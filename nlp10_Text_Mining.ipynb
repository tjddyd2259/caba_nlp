{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp10_Text Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjBuk7UfXtWpqaKYFOFm5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjddyd2259/caba_nlp/blob/main/nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se3ga5U66qzx",
        "outputId": "893527ec-e676-43e8-9df0-b4f2248028c6"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGsOCz7iDUaE",
        "outputId": "a77fd9a6-2124-43b9-b23b-6d4ef09b109f"
      },
      "source": [
        "from nltk import sent_tokenize\r\n",
        "text_sample = 'Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables.'\r\n",
        "sentences = sent_tokenize(text=text_sample)\r\n",
        "print(sentences)\r\n",
        "print(type(sentences)), len(sentences)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Regression analysis is primarily used for two conceptually distinct purposes.', 'First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.', 'Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables.']\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HznTf_sPErDU",
        "outputId": "2dc386e9-6a2f-4d87-ca6d-36046110c05c"
      },
      "source": [
        "# 단어 토큰화 (word_tokenize) : 공백, 콤마, 마침표, 개행문자, 정규표현식\r\n",
        "from nltk import word_tokenize\r\n",
        "\r\n",
        "sentences = 'Regression analysis is primarily used for two conceptually distinct purposes.'\r\n",
        "words = word_tokenize(sentences)\r\n",
        "print(words)\r\n",
        "print(type(words)),len(words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Regression', 'analysis', 'is', 'primarily', 'used', 'for', 'two', 'conceptually', 'distinct', 'purposes', '.']\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4285PwAhFMk4",
        "outputId": "d2767105-b806-4a70-cbe4-6bb7aab541db"
      },
      "source": [
        "# 문서에 대해서 모든 단어를 토큰화\r\n",
        "from nltk import sent_tokenize, word_tokenize\r\n",
        "\r\n",
        "def tokenize_text(text):\r\n",
        "  sentences = sent_tokenize(text) # 문장별 분리 토큰\r\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화 \r\n",
        "  return word_tokens\r\n",
        "\r\n",
        "word_tokens = tokenize_text(text_sample)\r\n",
        "print(word_tokens)\r\n",
        "print(type(word_tokens)) , len(word_tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Regression', 'analysis', 'is', 'primarily', 'used', 'for', 'two', 'conceptually', 'distinct', 'purposes', '.'], ['First', ',', 'regression', 'analysis', 'is', 'widely', 'used', 'for', 'prediction', 'and', 'forecasting', ',', 'where', 'its', 'use', 'has', 'substantial', 'overlap', 'with', 'the', 'field', 'of', 'machine', 'learning', '.'], ['Second', ',', 'in', 'some', 'situations', 'regression', 'analysis', 'can', 'be', 'used', 'to', 'infer', 'causal', 'relationships', 'between', 'the', 'independent', 'and', 'dependent', 'variables', '.']]\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q9YDTtiHUhW",
        "outputId": "a8ea6e7c-ca2d-420c-b9fa-8840e6113f4d"
      },
      "source": [
        "# 스톱워드 제거 : the , is , a , will 와 같이 문맥적으로 큰 의미가 없는 단어를 제거\r\n",
        "import nltk \r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q3XFcn7IFVP",
        "outputId": "b656112b-a181-4f11-c8a2-72e9d5220ad9"
      },
      "source": [
        "# NLTK english stopwords 갯수 확인\r\n",
        "print(len(nltk.corpus.stopwords.words('english')))\r\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmD587xTI41Y",
        "outputId": "8bbda300-1760-473c-89e7-af3bf5efab26"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\r\n",
        "import nltk\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "all_tokens = []\r\n",
        "for sentence in word_tokens:\r\n",
        "  filtered_words = []\r\n",
        "  for word in sentence:\r\n",
        "    word = word.lower()\r\n",
        "    if word not in stopwords:\r\n",
        "      filtered_words.append(word)\r\n",
        "  all_tokens.append(filtered_words)\r\n",
        "print(all_tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['regression', 'analysis', 'primarily', 'used', 'two', 'conceptually', 'distinct', 'purposes', '.'], ['first', ',', 'regression', 'analysis', 'widely', 'used', 'prediction', 'forecasting', ',', 'use', 'substantial', 'overlap', 'field', 'machine', 'learning', '.'], ['second', ',', 'situations', 'regression', 'analysis', 'used', 'infer', 'causal', 'relationships', 'independent', 'dependent', 'variables', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyWX8j6gKUmk",
        "outputId": "10e0f046-e358-40d8-f021-4aef6b4aaefd"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\r\n",
        "# Stemmer(LancasterStemmer)\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\r\n",
        "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\r\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fancist'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus aums amus\n",
            "fant fant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff-fmTt5Mykf",
        "outputId": "7b7256ad-03c8-413c-ed61-6127ad4d353f"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1FXAeFiLKaw",
        "outputId": "43171ba2-0389-453a-b108-fb1cb2c3000c"
      },
      "source": [
        "# Lemmatizetion(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "\r\n",
        "lemma = WordNetLemmatizer()\r\n",
        "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\r\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\r\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fancist','a'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse aumses amuse\n",
            "fancy fancist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcDkDrUMZSOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c65c76-b169-4a7f-d6ae-3c6e287ab83e"
      },
      "source": [
        "import numpy as np\r\n",
        "num_samples = 100\r\n",
        "height = 71\r\n",
        "width = 71\r\n",
        "num_classes = 100\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from keras.applications import Xception\r\n",
        "import datetime\r\n",
        "start = datetime.datetime.now()\r\n",
        "\r\n",
        "model = Xception(weights = None,\r\n",
        "                 input_shape =( height,width,3),\r\n",
        "                 classes = num_classes)\r\n",
        "model.compile(loss = 'categorical_crossentropy',\r\n",
        "              optimizer = 'rmsprop')\r\n",
        "x=np.random.random((num_samples,height,width,3))\r\n",
        "y=np.random.random((num_samples,num_classes))\r\n",
        "\r\n",
        "model.fit(x,y,epochs=3,batch_size=16)\r\n",
        "model.save('my_model.h5')\r\n",
        "end = datetime.datetime.now()\r\n",
        "time_delta = end - start"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7/7 [==============================] - 41s 176ms/step - loss: 233.5861\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 241.5049\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 243.9043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCH1942PodQP",
        "outputId": "4ff6e6c4-5959-44ec-c5e5-12c7691d2e20"
      },
      "source": [
        "print('걸린 시간: {}초'.format(time_delta.seconds))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "걸린 시간: 48초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpbQIjxfoGY8",
        "outputId": "b1d31162-4c19-4ea5-8ae0-bfd2b891bd01"
      },
      "source": [
        "import numpy as np\r\n",
        "num_samples = 100\r\n",
        "height = 71\r\n",
        "width = 71\r\n",
        "num_classes = 100\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from keras.applications import Xception\r\n",
        "import datetime\r\n",
        "start = datetime.datetime.now()\r\n",
        "\r\n",
        "with tf.device('/cpu:0'):\r\n",
        "\r\n",
        "  model = Xception(weights = None,\r\n",
        "                  input_shape =( height,width,3),\r\n",
        "                  classes = num_classes)\r\n",
        "  model.compile(loss = 'categorical_crossentropy',\r\n",
        "                optimizer = 'rmsprop')\r\n",
        "  x=np.random.random((num_samples,height,width,3))\r\n",
        "  y=np.random.random((num_samples,num_classes))\r\n",
        "\r\n",
        "model.fit(x,y,epochs=3,batch_size=16)\r\n",
        "model.save('my_model.h5')\r\n",
        "end = datetime.datetime.now()\r\n",
        "time_delta = end - start"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7/7 [==============================] - 8s 160ms/step - loss: 237.6417\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 1s 157ms/step - loss: 248.3512\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 1s 153ms/step - loss: 247.3586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy-yvAvMo6kG",
        "outputId": "5597ce1b-6a31-4e43-c404-70633c2fd6af"
      },
      "source": [
        "print('걸린 시간: {}초'.format(time_delta.seconds))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "걸린 시간: 11초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjOlcWcdpGx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf90eb5-cae3-4b64-b829-c05eb69e04ae"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSipwnVS3Q7V",
        "outputId": "5ebaf12a-695a-4885-b426-8aaa435edfb6"
      },
      "source": [
        "from konlpy.tag import Okt\r\n",
        "okt = Okt()\r\n",
        "print(okt.morphs('단독입찰보다 복수입찰의 경우'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm5R0_uk6B3c",
        "outputId": "1cd0a1bd-c36d-4248-f8c7-944b64b9443b"
      },
      "source": [
        "print(okt.nouns('유일하게 항공기 체계 종합개발 경험을 갖고 있는 kai는'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['항공기', '체계', '종합', '개발', '경험']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttvpzcy86mGd",
        "outputId": "a47db8a9-a7d0-4e75-da8a-4311a4474b22"
      },
      "source": [
        "print(okt.nouns('나는 프로젝트를 하고있는데 너무 어렵다'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['나', '프로젝트']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttvpiz8K622f",
        "outputId": "4644dbbe-7b51-46a4-8b78-fd44e112d2a2"
      },
      "source": [
        "print(okt.phrases('날카로운 분석과 신뢰감 있는 진행으로'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Elaay-CP8f5r",
        "outputId": "b2a160ea-17e2-48a2-8cef-13029bf4e870"
      },
      "source": [
        "print(okt.pos('이것도 되나욬ㅋㅋ',norm=True))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC-DHp6K7Aa2",
        "outputId": "9263b1a1-8e3e-446d-9271-1498d8503c35"
      },
      "source": [
        "print(okt.pos('이것도 되나욬ㅋㅋ',norm=True,stem=True))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E3BZeTR7n6G",
        "outputId": "934b51a4-9052-4ad8-bd2e-af89b2d3805f"
      },
      "source": [
        "print(okt.pos('이것도 되나욬ㅋㅋ',norm=True,stem=True,join=True))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['이/Determiner', '것/Noun', '도/Josa', '되다/Verb', 'ㅋㅋ/KoreanParticle']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQf16Czm8kJk",
        "outputId": "b66f502f-f1d1-4e06-b2b0-feac2985f110"
      },
      "source": [
        "print(okt.pos('아름다운 꽃과 파란 하늘',norm=True))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('아름다운', 'Adjective'), ('꽃', 'Noun'), ('과', 'Josa'), ('파란', 'Noun'), ('하늘', 'Noun')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNxle-on84k9"
      },
      "source": [
        "a = okt.pos('아름다운 꽃과 파란 하늘')\r\n",
        "b = []\r\n",
        "c = []\r\n",
        "for i in range(len(a)):\r\n",
        "  if a[i][1] == 'Adjective':\r\n",
        "    b.append(a[i][0])\r\n",
        "  else :\r\n",
        "    c.append(a[i][0])\r\n",
        "\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkWG8xdb-O-K",
        "outputId": "55a6ae8d-fe29-4f2a-ec12-5360e4e64553"
      },
      "source": [
        "b = []\r\n",
        "for i,j in a:\r\n",
        "  if j == 'Noun':\r\n",
        "    b.append(i)\r\n",
        "b"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['꽃', '파란', '하늘']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18C-Ww7X_ZIE"
      },
      "source": [
        "list1 = okt.nouns('나는 오늘 방콕에 가고싶다.')\r\n",
        "list2 = okt.pos('나는 오늘 방콕에 갔다.',norm=True,stem=True)\r\n",
        "list3 = okt.morphs('친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다')\r\n",
        "list4 = okt.pos('나는 오늘도 장에 가고싶다',norm=True,stem=True,join=True)\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUaobm9DA46S",
        "outputId": "3920b9da-f929-494a-dc75-b3f7f282c23f"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\r\n",
        "\r\n",
        "news_data = fetch_20newsgroups(subset='all',random_state=0)\r\n",
        "news_data.keys()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD4n-aaLDnAD",
        "outputId": "81340801-c099-4bb4-d2fc-8e18895c3122"
      },
      "source": [
        "import pandas as pd\r\n",
        "print(news_data.target)\r\n",
        "a = pd.Series(news_data.target).unique()\r\n",
        "sorted(a)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6  1 15 ...  0  5  8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOr8zl2WEBVc",
        "outputId": "0d878ade-03ef-4360-874b-0b789606dd68"
      },
      "source": [
        "train_news = fetch_20newsgroups(subset='train',remove=('header','footer','quotes'),random_state=0)\r\n",
        "X_train = train_news.data\r\n",
        "y_train = train_news.target\r\n",
        "test_news = fetch_20newsgroups(subset='test',remove=('header','footer','quotes'),random_state=0)\r\n",
        "X_test = test_news.data\r\n",
        "y_test = test_news.target\r\n",
        "print(len(X_train),len(X_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11314 7532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdKaHibBHbmT",
        "outputId": "f29d7a51-9c23-4fad-e1fa-d71cb227c178"
      },
      "source": [
        "print(news_data.target_names)\r\n",
        "print(pd.Series(y_test).value_counts().sort_index())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "0     319\n",
            "1     389\n",
            "2     394\n",
            "3     392\n",
            "4     385\n",
            "5     395\n",
            "6     390\n",
            "7     396\n",
            "8     398\n",
            "9     397\n",
            "10    399\n",
            "11    396\n",
            "12    393\n",
            "13    396\n",
            "14    394\n",
            "15    398\n",
            "16    364\n",
            "17    376\n",
            "18    310\n",
            "19    251\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paSnlZohInT8",
        "outputId": "31a20c57-a391-4aab-a5ba-586e69a1220a"
      },
      "source": [
        "# 피처 벡터화 변환\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "cnt_vect = CountVectorizer()\r\n",
        "cnt_vect.fit(X_train)\r\n",
        "X_train_cnt_vect = cnt_vect.transform(X_train)\r\n",
        "# 학습 데이터로 fit()된 Countervectorizer를 이용, 테스트 데이터 피처 벡터화 변환 \r\n",
        "# (피처 개수가 동일해야 함)\r\n",
        "X_test_cnt_vect = cnt_vect.transform(X_test)\r\n",
        "print(X_train_cnt_vect.shape)\r\n",
        "print(X_test_cnt_vect.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 120756)\n",
            "(7532, 120756)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACLscHNWKEXE",
        "outputId": "86f5142e-34db-4402-ab8d-d41539c1e446"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "lr = LogisticRegression()\r\n",
        "lr.fit(X_train_cnt_vect,y_train)\r\n",
        "pred = lr.predict(X_test_cnt_vect)\r\n",
        "accuracy_score(y_test,pred)\r\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7550451407328731"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BajjlU0fM5oJ",
        "outputId": "923b49f8-b3e1-4232-ed47-48cd05ee5c6e"
      },
      "source": [
        "# 피처 벡터화 변환 : TF-IDF 벡터화\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "tfidf_vect = TfidfVectorizer()\r\n",
        "tfidf_vect.fit(X_train)\r\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\r\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\r\n",
        "\r\n",
        "lr = LogisticRegression()\r\n",
        "lr.fit(X_train_tfidf_vect,y_train)\r\n",
        "pred = lr.predict(X_test_tfidf_vect)\r\n",
        "accuracy_score(y_test,pred)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7841210833775889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jkhUhvTOgIX",
        "outputId": "883588ff-f9d9-4fd3-852c-5232a0748186"
      },
      "source": [
        "# stop words 필터링 추가 , ngram을 기본(1,1)에서 (1,2)로 max_df=300 으로 변경해 피처 벡터화 적용\r\n",
        "tfidf_vect = TfidfVectorizer(stop_words='english',ngram_range=(1,2),max_df=300)\r\n",
        "tfidf_vect.fit(X_train)\r\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\r\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\r\n",
        "\r\n",
        "lr = LogisticRegression()\r\n",
        "lr.fit(X_train_tfidf_vect,y_train)\r\n",
        "pred = lr.predict(X_test_tfidf_vect)\r\n",
        "accuracy_score(y_test,pred)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.774429102496017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhLZPjY8QJn4",
        "outputId": "d928da89-4ba4-464c-eb37-61489621d8cc"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "# 최적 c값 도출 튜닝 수행 , cv 는 3 Fold셋으로 설정\r\n",
        "params = {'C':[5,10]}\r\n",
        "gcv_lr = GridSearchCV(lr,param_grid=params,cv=3,scoring='accuracy',verbose=1)\r\n",
        "gcv_lr.fit(X_train_tfidf_vect,y_train)\r\n",
        "print(gcv_lr.best_params_)\r\n",
        "lr_pred = gcv_lr.predict(X_test_tfidf_vect)\r\n",
        "print(accuracy_score(y_test,lr_pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 36.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 10}\n",
            "0.7980616038236856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc1IERzESA1g"
      },
      "source": [
        "# 사이킷런 파이프라인 \r\n",
        "# TfidfVectorizer 객체를 tfidf_vect 객체명으로 LogisticRegression 객체를 lr_clf 객체명으로 생성하는 \r\n",
        "# pipeline 생성\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "pipeline = Pipeline([\r\n",
        "                     ('tfidf_vect',TfidfVectorizer(stop_words='english',\r\n",
        "                                                   ngram_range=(1,2),max_df=300)),\r\n",
        "                     ('lr',LogisticRegression(C=10))])\r\n",
        "pipeline.fit(X_train,y_train)\r\n",
        "pred = pipeline.predict(X_test)\r\n",
        "print(accuracy_score(y_test,pred))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjB_tBZRX2-G"
      },
      "source": [
        "# 사이킷런 파이프라인과   GridSearchCV와의 결합 \r\n",
        "pipeline = Pipeline([\r\n",
        "                     ('tfidf_vect',TfidfVectorizer(stop_words='english')),\r\n",
        "                     ('lr',LogisticRegression())\r\n",
        "])\r\n",
        "params = {'tfidf_vect__ngram_range':[(1,1),(1,2),(1,3)],\r\n",
        "          'tfidf_vect__max_df':[100,300,700],\r\n",
        "          'lr__C':[1,5,10]}\r\n",
        "grid_cv_pipe = GridSearchCV(pipeline,param_grid=params,cv=3,scoring='accuracy',verbose=1)\r\n",
        "grid_cv_pipe.fit(X_train,y_train)\r\n",
        "print(grid_cv_pipe.best_params_,grid_cv_pipe.best_score_)\r\n",
        "pred = grid_cv_pipe.predict(X_test)\r\n",
        "print(accuracy_score(y_test,pred))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}