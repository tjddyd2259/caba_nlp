{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp10_Text Mining.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPltYvHHaBOJDPcdFOZt1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjddyd2259/caba_nlp/blob/main/nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se3ga5U66qzx",
        "outputId": "37baf2cc-cec3-4380-e9c0-cc8c101122cb"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGsOCz7iDUaE",
        "outputId": "fb786d83-2fcc-4362-deb9-62d0e0b95d45"
      },
      "source": [
        "from nltk import sent_tokenize\r\n",
        "text_sample = 'Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables.'\r\n",
        "sentences = sent_tokenize(text=text_sample)\r\n",
        "print(sentences)\r\n",
        "print(type(sentences)), len(sentences)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Regression analysis is primarily used for two conceptually distinct purposes.', 'First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.', 'Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables.']\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HznTf_sPErDU",
        "outputId": "c74d5025-e755-460b-923c-0eb178a440a6"
      },
      "source": [
        "# 단어 토큰화 (word_tokenize) : 공백, 콤마, 마침표, 개행문자, 정규표현식\r\n",
        "from nltk import word_tokenize\r\n",
        "\r\n",
        "sentences = 'Regression analysis is primarily used for two conceptually distinct purposes.'\r\n",
        "words = word_tokenize(sentences)\r\n",
        "print(words)\r\n",
        "print(type(words)),len(words)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Regression', 'analysis', 'is', 'primarily', 'used', 'for', 'two', 'conceptually', 'distinct', 'purposes', '.']\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4285PwAhFMk4",
        "outputId": "8b7811ee-6711-4b55-971a-fe6b5cd4e1e9"
      },
      "source": [
        "# 문서에 대해서 모든 단어를 토큰화\r\n",
        "from nltk import sent_tokenize, word_tokenize\r\n",
        "\r\n",
        "def tokenize_text(text):\r\n",
        "  sentences = sent_tokenize(text) # 문장별 분리 토큰\r\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화 \r\n",
        "  return word_tokens\r\n",
        "\r\n",
        "word_tokens = tokenize_text(text_sample)\r\n",
        "print(word_tokens)\r\n",
        "print(type(word_tokens)) , len(word_tokens)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Regression', 'analysis', 'is', 'primarily', 'used', 'for', 'two', 'conceptually', 'distinct', 'purposes', '.'], ['First', ',', 'regression', 'analysis', 'is', 'widely', 'used', 'for', 'prediction', 'and', 'forecasting', ',', 'where', 'its', 'use', 'has', 'substantial', 'overlap', 'with', 'the', 'field', 'of', 'machine', 'learning', '.'], ['Second', ',', 'in', 'some', 'situations', 'regression', 'analysis', 'can', 'be', 'used', 'to', 'infer', 'causal', 'relationships', 'between', 'the', 'independent', 'and', 'dependent', 'variables', '.']]\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q9YDTtiHUhW",
        "outputId": "9a7c7f26-b65e-42cd-9fb9-780b2194086b"
      },
      "source": [
        "# 스톱워드 제거 : the , is , a , will 와 같이 문맥적으로 큰 의미가 없는 단어를 제거\r\n",
        "import nltk \r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q3XFcn7IFVP",
        "outputId": "4044c2a3-7aac-443f-b51e-4009973b684d"
      },
      "source": [
        "# NLTK english stopwords 갯수 확인\r\n",
        "print(len(nltk.corpus.stopwords.words('english')))\r\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmD587xTI41Y",
        "outputId": "905fe83a-5cc1-44d4-ba34-e384977ced13"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\r\n",
        "import nltk\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "all_tokens = []\r\n",
        "for sentence in word_tokens:\r\n",
        "  filtered_words = []\r\n",
        "  for word in sentence:\r\n",
        "    word = word.lower()\r\n",
        "    if word not in stopwords:\r\n",
        "      filtered_words.append(word)\r\n",
        "  all_tokens.append(filtered_words)\r\n",
        "print(all_tokens)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['regression', 'analysis', 'primarily', 'used', 'two', 'conceptually', 'distinct', 'purposes', '.'], ['first', ',', 'regression', 'analysis', 'widely', 'used', 'prediction', 'forecasting', ',', 'use', 'substantial', 'overlap', 'field', 'machine', 'learning', '.'], ['second', ',', 'situations', 'regression', 'analysis', 'used', 'infer', 'causal', 'relationships', 'independent', 'dependent', 'variables', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyWX8j6gKUmk",
        "outputId": "14b1f04c-e7d8-411d-a5b1-227ef29956fc"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\r\n",
        "# Stemmer(LancasterStemmer)\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\r\n",
        "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\r\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fancist'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus aums amus\n",
            "fant fant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff-fmTt5Mykf",
        "outputId": "310a6a1f-30bf-4c0c-a0f0-e78f9d47f281"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1FXAeFiLKaw",
        "outputId": "c1e7223c-9e0d-4d48-b866-93b4b6b73cad"
      },
      "source": [
        "# Lemmatizetion(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "\r\n",
        "lemma = WordNetLemmatizer()\r\n",
        "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\r\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\r\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fancist','a'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse aumses amuse\n",
            "fancy fancist\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}